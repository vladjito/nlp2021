{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Імпортування бібліотек\n"
      ],
      "metadata": {
        "id": "NiVdMoogytBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rfrnDDpwFm_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "import pickle\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, Flatten, Reshape, concatenate, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визначення допоміжних функцій"
      ],
      "metadata": {
        "id": "LAXWRuB3y3rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load descriptions\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "    \n",
        "  \n",
        "def load_descriptions(doc):\n",
        "    mapping = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        image_id = image_id.split('.')[0]\n",
        "        image_desc = ' '.join(image_desc)\n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id] = list()\n",
        "        mapping[image_id].append(image_desc)\n",
        "    return mapping\n",
        "  \n",
        "def clean_descriptions(descriptions):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            desc = desc.split()\n",
        "            desc = [word.lower() for word in desc]\n",
        "            desc = [w.translate(table) for w in desc]\n",
        "            desc = [word for word in desc if len(word)>1]\n",
        "            desc = [word for word in desc if word.isalpha()]\n",
        "            desc_list[i] =  ' '.join(desc)\n",
        "            \n",
        "     return descriptions\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        if image_id in dataset:\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            descriptions[image_id].append(desc)\n",
        "    return descriptions\n",
        "  \n",
        "def load_set(filename):\n",
        "    doc = load_doc(filename)\n",
        "    dataset = list()\n",
        "    for line in doc.split('\\n'):\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        identifier = line.split('.')[0]\n",
        "        dataset.append(identifier)\n",
        "    return set(dataset)\n",
        "\n",
        "# load training dataset \n",
        "\n",
        "\n",
        "filename = \"dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
        "doc = load_doc(filename)\n",
        "descriptions = load_descriptions(doc)\n",
        "descriptions = clean_descriptions(descriptions)\n",
        "save_descriptions(descriptions, 'descriptions.txt')\n",
        "filename = 'dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)"
      ],
      "metadata": {
        "id": "g0tWrcQPwQWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ембедінг"
      ],
      "metadata": {
        "id": "c4PTncoRzIMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of all the training captions\n",
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)\n",
        "        \n",
        "        \n",
        "# Consider only words which occur at least 10 times in the corpus\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "print('Preprocessed words {} -> {}'.format(len(word_counts), len(vocab)))\n",
        "\n",
        "\n",
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "    \n",
        "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
        "\n",
        "# Load Glove vectors\n",
        "glove_dir = 'glove.6B'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_dim = 200\n",
        "\n",
        "# Get 200-dim dense vector for each of the words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in wordtoix.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "zP7CtW_fzNXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Підготовка даних"
      ],
      "metadata": {
        "id": "q0KS2WMP03rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below path contains all the images\n",
        "all_images_path = 'dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
        "# Create a list of all image names in the directory\n",
        "all_images = glob.glob(all_images_path + '*.jpg')\n",
        "\n",
        "# Create a list of all the training and testing images with their full path names\n",
        "def create_list_of_images(file_path):\n",
        "  images_names = set(open(file_path, 'r').read().strip().split('\\n'))\n",
        "  images = []\n",
        "\n",
        "  for image in all_images: \n",
        "      if image[len(all_images_path):] in image_names:\n",
        "          images.append(image)\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "train_images_path = 'dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "test_images_path = 'dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "\n",
        "train_images = create_list_of_images(train_images_path)\n",
        "test_images = create_list_of_images(test_images_path)\n",
        "\n",
        "#preprocessing the images\n",
        "def preprocess(image_path):\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "# Load the inception v3 model\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
        "model_new = Model(model.input, model.layers[-2].output)\n",
        "\n",
        "# Encoding a given image into a vector of size (2048, )\n",
        "def encode(image):\n",
        "    image = preprocess(image) \n",
        "    fea_vec = model_new.predict(image) \n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
        "    return fea_vec\n",
        "  \n",
        "  \n",
        "encoding_train = {}\n",
        "for img in train_images:\n",
        "    encoding_train[img[len(all_images_path):]] = encode(img)\n",
        "    \n",
        "    \n",
        "encoding_test = {}\n",
        "for img in test_images:\n",
        "    encoding_test[img[len(all_images_path):]] = encode(img)\n",
        "    \n",
        "# Save the bottleneck features to disk\n",
        "with open(\"encoded_files/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding_train, encoded_pickle)\n",
        "    \n",
        "with open(\"encoded_files/encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding_test, encoded_pickle)\n",
        "    \n",
        "    \n",
        "train_features = load(open(\"encoded_files/encoded_train_images.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "eA9b1CI306PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key+'.jpg']\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            # yield the batch data\n",
        "            if n==num_photos_per_batch:\n",
        "                yield [[array(X1), array(X2)], array(y)]\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "metadata": {
        "id": "6_836iqj1F1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Архітектура моделі та тренування"
      ],
      "metadata": {
        "id": "tpT0fetb3VDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length1,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "epochs = 20\n",
        "number_pics_per_batch = 3\n",
        "steps = len(train_descriptions)//number_pics_per_batch\n",
        "\n",
        "generator = data_generator(train_descriptions, train_features, wordtoix, max_length1, number_pics_per_batch)\n",
        "history = model.fit_generator(generator, epochs=20,  steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "\n",
        "model.optimizer.lr = 0.0001\n",
        "epochs = 10\n",
        "number_pics_per_batch = 6\n",
        "steps = len(train_descriptions)//number_pics_per_batch\n",
        "\n",
        "generator = data_generator(train_descriptions, train_features, wordtoix, max_length1, number_pics_per_batch)\n",
        "history1 = model.fit_generator(generator, epochs=10, steps_per_epoch=steps, verbose=1)\n",
        "model.save('saved_model/model_' + str(30) + '.h5')"
      ],
      "metadata": {
        "id": "uZUJFw9P3Xmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Жадібна функція"
      ],
      "metadata": {
        "id": "0tgb5jBK5p2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length1):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length1)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final\n",
        "    \n",
        "z=1\n",
        "pic = list(encoding_test.keys())[999]\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(images+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "print(\"Greedy:\",greedySearch(image))"
      ],
      "metadata": {
        "id": "g8fvL8om5q94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}